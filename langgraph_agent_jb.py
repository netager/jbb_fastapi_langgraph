import os
import json
from typing import List, Annotated, AsyncGenerator
from typing_extensions import TypedDict
from urllib.parse import urlencode

from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama, OllamaEmbeddings
from langchain_teddynote.messages import messages_to_history
from langchain_teddynote.tools.tavily import TavilySearch
from langchain_teddynote.evaluator import GroundednessChecker
from langgraph.checkpoint.memory import MemorySaver

from rag.utils import format_docs
from rag.pdf import PDFRetrievalChain

from dotenv import load_dotenv
load_dotenv()

from langchain_teddynote import logging
logging.langsmith("CH17-LangGraph-Structures")

# PDF ë¬¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
# pdf = PDFRetrievalChain(["data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf"]).create_chain()
pdf = PDFRetrievalChain().create_chain()

# retrieverì™€ chainì„ ìƒì„±í•©ë‹ˆë‹¤.
pdf_retriever = pdf.retriever
pdf_chain = pdf.chain

# # ObtÃ©m a chave da API do ambiente
# api_key = os.getenv("OPENAI_API_KEY")

class State(TypedDict):
    question: Annotated[str, "Question"]  # ì§ˆë¬¸(ëˆ„ì ë˜ëŠ” list)
    context: Annotated[list, "Context"]  # ë¬¸ì„œì˜ ê²€ìƒ‰ ê²°ê³¼
    answer: Annotated[str, "Answer"]  # ë‹µë³€
    messages: Annotated[list, add_messages]  # ë©”ì‹œì§€(ëˆ„ì ë˜ëŠ” list)
    relevance: Annotated[str, "Relevance"]  # ê´€ë ¨ì„±


def print_state(state, func_name) -> None:
    print(f"- [{func_name}]-------------------------------------")
    print(f"question: {state['question']}")
    # print(f"context: {type(state['context'])}, {len(state['context'])}, {state['context']}")
    for context in state['context']:
        # print(f"context: {context['page_context']}")
        print(f"context: {context.metadata['source']}")

    # print(f"answer: {state['answer'][:20]}")
    print(f"messages: {state['messages'][-1]}")
    print(f"relevance: {state['relevance']}")
    print("--------------------------------------")

# ë¬¸ì„œ ê²€ìƒ‰ ë…¸ë“œ
def retrieve_document(state: State) -> State:
    
    print_state(state, 'retrieve_document')

    # print(f"[langgraph_agent_jb.py] state : {state['messages']}")        

    # ì§ˆë¬¸ì„ ìƒíƒœì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.
    latest_question = state["question"]

    # ë¬¸ì„œì—ì„œ ê²€ìƒ‰í•˜ì—¬ ê´€ë ¨ì„± ìˆëŠ” ë¬¸ì„œë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    retrieved_docs = pdf_retriever.invoke(latest_question)
    # print(f'[langgraph_agent_jp][retrieve_document] {latest_question}')
    # print('-' * 60)
    # print(f'[langgraph_agent_jp][retrieve_document] {retrieved_docs}')   # TODO: Delete

    # ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í˜•ì‹í™”í•©ë‹ˆë‹¤.(í”„ë¡¬í”„íŠ¸ ì…ë ¥ìœ¼ë¡œ ë„£ì–´ì£¼ê¸° ìœ„í•¨)
    # retrieved_docs = format_docs(retrieved_docs)

    # ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ context í‚¤ì— ì €ì¥í•©ë‹ˆë‹¤.
    return State(context=retrieved_docs)

        
def llm_answer(state: State) -> State:
    print_state(state, 'llm_answer')

    latest_question = state["question"]
    context = format_docs(state["context"])
    # context = state["context"]

    # print(f'[langgraph_agent_jp][llm_answer()] <{latest_question}>{type(context)}')
    # print(f'[langgraph_agent_jp][llm_answer()] context: {context}')

    # last_question = "ì •ë³´ë³´í˜¸ìœ„ì›íšŒ ìœ„ì›ì— ëŒ€í•´ ì•Œë ¤ì¤˜."
    
    response = pdf_chain.invoke(
        {
            "question": latest_question,
            "context": context,
            # "chat_history": messages_to_history(state["messages"]),
            "chat_history": []
        }
    )

    search_results = state['context']
    print(f"[langgraph_agent_jb][llm_answer()] search_results: {search_results}")

    # if not search_results:
    #     logger.warning("No relevant documents found in search results.")
            
    linked_docs = []
    base_url = "https://jabis.jbbank.co.kr/jabis_pdf_view"
    
    for search_result in search_results:
        # if search_result[1] < 0.8:  # relevance threshold
        params = {
            "source": search_result.metadata["source"],
            "title": search_result.metadata["title"],
            "page": search_result.metadata["page"] + 1,
        }
        url_with_params = base_url + "?" + urlencode(params)
        
        linked_docs.append(
            f"ğŸ‘‰ [{params['title']}]({url_with_params}) [pages]: {params['page']}"
        )

    # print(f"[langgraph_agent_jb][llm_answer()] linked_docs: {linked_docs}")
       
    response = response + "\n\n ğŸ“– ê´€ë ¨ ë¬¸ì„œ ë³´ê¸°\n\n" + "\n\n".join(linked_docs)

    print(f"response: {response}")
    # ìƒì„±ëœ ë‹µë³€, (ìœ ì €ì˜ ì§ˆë¬¸, ë‹µë³€) ë©”ì‹œì§€ë¥¼ ìƒíƒœì— ì €ì¥í•©ë‹ˆë‹¤.
    return {
        "answer": response,
        "messages": [("assistant", response)]
    }

    # return {
    #     "answer": response,
    #     "messages": [("user", latest_question), ("assistant", response)]
    # }

    # return {"messages": [response]}        
    # return {"messages": [llm.invoke(state["messages"])]}

# ê·¸ë˜í”„ ì •ì˜
graph_builder = StateGraph(State)

# ë…¸ë“œ ì •ì˜
graph_builder.add_node("retrieve", retrieve_document)
graph_builder.add_node("llm_answer", llm_answer)

# ì—£ì§€ ì •ì˜
graph_builder.add_edge(START, "retrieve")
graph_builder.add_edge("retrieve", "llm_answer")
graph_builder.add_edge("llm_answer", END)

# ì²´í¬í¬ì¸í„° ì„¤ì •
memory = MemorySaver()

# ì»´íŒŒì¼
graph = graph_builder.compile()